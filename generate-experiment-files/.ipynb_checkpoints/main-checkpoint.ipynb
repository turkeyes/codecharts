{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental set-up: ##\n",
    "\n",
    "This code will generate experimental files that can either be independently hosted on a website and run with recruited participants, or via our [MTurk iPython notebook](https://github.com/a-newman/mturk-api-notebook) be used for launching Amazon Mechanical Turk (MTurk) HITs. \n",
    "\n",
    "An experiment is composed of different sets of images:\n",
    "* **target images** are the images you want to collect attention data on - those are images that you provide (in directory `sourcedir` below)\n",
    "* **tutorial images** are images that will be shown to participants at the beginning of the experiment to get them familiarized with the codecharts set-up (you can reuse the tutorial image we provide, or provide your own in directory `tutorial_source_dir` below)\n",
    "    * *hint: if your images are very different in content from the images in our set, you may want to train your participants on your own images, to avoid a context switch between the tutorial and main experiment*\n",
    "* **sentinel images** are images interspersed throughout the experiment where participant attention is guided to a very specific point on the screen, used as validation/calibration images to ensure participants are actually moving their eyes and looking where they're supposed to; the code below will intersperse images from the `sentinel_target_images` directory we provide throughout your experimental sequence\n",
    "    * sentinel images can be interspersed throughout both the tutorial and target images, or excluded from the tutorial (via `add_sentinels_to_tutorial` flag below); we recommend having sentinel images as part of the tutorial to familiarize participants with such images as well\n",
    "    \n",
    "The code below will populate the `rootdir` task directory with #`num_subject_files` subject files for you, where each subject file corresponds to an experiment you can run on a single participant. For each subject file, a set of #`num_images_per_sf` will be randomly sampled from the `sourcedir` image directory. A set of #`num_sentinels_per_sf` sentinel images will also be sampled from the `sentinel_imdir` image directory, and distributed throughout the experiment. A tutorial will be generated at the beginning of the experiment with #`num_imgs_per_tutorial` randomly sampled from the `tutorial_source_dir` image directory, along with an additional #`num_sentinels_per_tutorial` sentinel files distributed throughout the tutorial (if `add_sentinels_to_tutorial` flag is set to true). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import random\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import base64 \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcedir = '../demo_experiment_images/' # replace this with your own directory of experiment images\n",
    "tutorial_source_dir = 'tutorial_images'  # you can reuse the tutorial images we provide, or provide your own directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS for generating subject files\n",
    "num_subject_files = 3     # number of subject files to generate (i.e., # of mturk assignments that will be put up)    \n",
    "num_images_per_sf = 35    # number of target images per subject file \n",
    "num_imgs_per_tutorial = 3 # number of tutorial images per subject file\n",
    "num_sentinels_per_sf = 5  # number of sentinel images to distribute throughout the experiment (excluding the tutorial)\n",
    "add_sentinels_to_tutorial = True # whether to have sentinel images as part of the tutorial\n",
    "num_sentinels_per_tutorial = 3   # number of sentinel images to distribute throughout the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another bit of terminology and experimental logistics involves **buckets** which are a way to distribute experiment stimuli so that multiple experiments can be run in parallel (and participants can be reused for different subsets of images). If you have a lot of images that you want to collect data on, and for each participant you are sampling a set of only #`num_images_per_sf`, then you might have to generate a large `num_subject_files` in order to have enough data points per image. A way to speed up data collection is to split all the target images into #`num_buckets` disjoint buckets, and then to generate subject files per bucket. Given that subject files generated per bucket are guaranteed to have a disjoint set of images, the same participant can be run on multiple subject files from different buckets. MTurk HITs corresponding to different buckets can be launched all at once. In summary, in MTurk terms, you can generate as many HITs as `num_buckets` specified below, and as many assignments per HIT as `num_subject_files`. \n",
    "\n",
    "The way the codecharts methodology works, a jittered grid of alphanumeric triplets appears after every image presentation (whether it is a target, sentinel, or tutorial image), since a participant will need to indicate where on the preceding image s/he looked, by reporting a triplet. To avoid generating an excessive number of codecharts (that bulks up all the subject files), we can reuse some codecharts across buckets. The way we do this is by pre-generating #`ncodecharts` codecharts, and then randomly sampling from these when generating the individual subject files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory ../assets/task_data\n"
     ]
    }
   ],
   "source": [
    "# we pre-generate some codecharts and sentinel images so that we can reuse these across participants and buckets \n",
    "# and potentially not have to generate as many files; these can be set to any number, and the corresponding code\n",
    "# will just sample as many images as need per subject file\n",
    "\n",
    "ncodecharts = num_subject_files*num_images_per_sf # number of codecharts to generate; can be changed\n",
    "sentinel_images_per_bucket = num_subject_files*num_sentinels_per_sf # can be changed\n",
    "\n",
    "# set these parameters\n",
    "num_buckets = 1      # number of disjoint sets of subject files to create (for running multiple parallel HITs)\n",
    "start_bucket_at = 0  # you can use this and the next parameter to generate more buckets if running the code later\n",
    "which_buckets = [0]  # a list of specific buckets e.g., [4,5,6] to generate experiment data for\n",
    "\n",
    "rootdir = '../assets/task_data' # where all the experiment data will be stored\n",
    "if not os.path.exists(rootdir):\n",
    "    print('Creating directory %s'%(rootdir))\n",
    "    os.makedirs(rootdir)\n",
    "\n",
    "real_image_dir = os.path.join(rootdir,'real_images')              # target images, distributed by buckets\n",
    "real_CC_dir = os.path.join(rootdir,'real_CC')                     # codecharts corresponding to the target images \n",
    "                                                                  # (shared across buckets)\n",
    "sentinel_image_dir = os.path.join(rootdir,'sentinel_images')      # sentinel images, distributed by buckets\n",
    "sentinel_CC_dir = os.path.join(rootdir,'sentinel_CC')             # codecharts corresponding to the sentinel images\n",
    "                                                                  # (shared across buckets)\n",
    "#sentinel_targetim_dir = os.path.join(rootdir, 'sentinel_target')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory ../assets/task_data/all_images\n",
      "35 files copied from ../demo_experiment_images/ to ../assets/task_data/all_images\n",
      "Padding 35 image files to dimensions: [1920,1080]...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# this cell creates an `all_images` directory, copies images from sourcedir, and pads them to the required dimensions\n",
    "\n",
    "import create_padded_image_dir\n",
    "\n",
    "all_image_dir = os.path.join(rootdir,'all_images')\n",
    "if not os.path.exists(all_image_dir):\n",
    "    print('Creating directory %s'%(all_image_dir))\n",
    "    os.makedirs(all_image_dir)\n",
    "    \n",
    "allfiles = []\n",
    "for ext in ('*.jpeg', '*.png', '*.jpg'):\n",
    "    allfiles.extend(glob.glob(os.path.join(sourcedir, ext)))\n",
    "print(\"%d files copied from %s to %s\"%(len(allfiles),sourcedir,all_image_dir))\n",
    "    \n",
    "image_width,image_height = create_padded_image_dir.save_padded_images(all_image_dir,allfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using font size: 30\n",
      "Saved fixation cross image as ../assets/task_data/fixation-cross.jpg\n"
     ]
    }
   ],
   "source": [
    "# this cell generates a central fixation cross the size of the required image dimensions\n",
    "# it is a gray image with a white cross in the middle that is supposed to re-center participant gaze, and provide a\n",
    "# momentary break, between consecutive images\n",
    "\n",
    "from generate_central_fixation_cross import save_fixation_cross\n",
    "\n",
    "save_fixation_cross(rootdir,image_width,image_height);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributing images across 1 buckets\n",
      "Populating ../assets/task_data/real_images/bucket0 with 35 images\n"
     ]
    }
   ],
   "source": [
    "# this cell creates the requested number of buckets and distributes images from `all_image_dir` to the corresponding\n",
    "# bucket directories inside `real_image_dir`\n",
    "\n",
    "from distribute_image_files_by_buckets import distribute_images\n",
    "\n",
    "distribute_images(all_image_dir,real_image_dir,num_buckets,start_bucket_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 105 codecharts...\n",
      "0/105\n",
      "100/105\n",
      "Writing out ../assets/task_data/real_CC/CC_codes.json\n",
      "Writing out ../assets/task_data/real_CC/CC_codes_full.json\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# this cell generates #ncodecharts \"codecharts\" (jittered grids of triplets) of the required image dimensions\n",
    "\n",
    "import generate_codecharts \n",
    "from create_codecharts_dir import create_codecharts\n",
    "\n",
    "create_codecharts(real_CC_dir,ncodecharts,image_width,image_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create sentinel images by taking a small object (one of a: fixation cross, red dot, or image of a face) and choosing a random location for it on a blank image (away from the image boundaries by at least `border_padding` pixels). The code below creates #`sentinel_images_per_bucket` such sentinel images in each bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating ../assets/task_data/sentinel_images/bucket0 with 15 sentinel images\n",
      "Populating ../assets/task_data/sentinel_CC/bucket0 with 15 corresponding codecharts\n",
      "Writing out ../assets/task_data/sentinel_images/bucket0/sentinel_codes.json\n",
      "Writing out ../assets/task_data/sentinel_images/bucket0/sentinel_codes_full.json\n"
     ]
    }
   ],
   "source": [
    "# this cell generates #sentinel_images_per_bucket sentinel images per bucket, along with the corresponding codecharts\n",
    "\n",
    "import generate_sentinels\n",
    "\n",
    "# settings for generating sentinels\n",
    "sentinel_type = \"img\" # one of 'fix_cross', 'red_dot', or 'img'\n",
    "sentinel_imdir = 'sentinel_target_images' # directory where to find face images to use for generating sentinel images\n",
    "                                          # only relevant if sentinel_type=\"img\"\n",
    "\n",
    "border_padding = 100 # used to guarantee that chosen sentinel location is not too close to border to be hard to spot\n",
    "\n",
    "generate_sentinels.generate_sentinels(sentinel_image_dir,sentinel_CC_dir,num_buckets,start_bucket_at,sentinel_images_per_bucket,\\\n",
    "                       image_width,image_height,border_padding,sentinel_type,sentinel_imdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory ../assets/task_data/tutorial_images\n",
      "Padding 9 image files to dimensions: [1920,1080]...\n",
      "Done!\n",
      "A total of 9 images will be sampled from for the tutorials.\n",
      "Populating ../assets/task_data/tutorial_sentinels with 6 sentinel images\n",
      "Populating ../assets/task_data/tutorial_CC with 6 corresponding codecharts\n",
      "Writing out ../assets/task_data/tutorial.json\n",
      "Writing out ../assets/task_data/tutorial_full.json\n"
     ]
    }
   ],
   "source": [
    "# this cell generates codecharts corresponding to tutorial images, as well as sentinel images for the tutorial\n",
    "\n",
    "from generate_tutorials import generate_tutorials\n",
    "\n",
    "# inherit border_padding and sentinel type from above cell\n",
    "\n",
    "tutorial_image_dir = os.path.join(rootdir,'tutorial_images') # where processed tutorial images will be saved\n",
    "if not os.path.exists(tutorial_image_dir):\n",
    "    print('Creating directory %s'%(tutorial_image_dir))\n",
    "    os.makedirs(tutorial_image_dir)\n",
    "    \n",
    "allfiles = []\n",
    "for ext in ('*.jpeg', '*.png', '*.jpg'):\n",
    "    allfiles.extend(glob.glob(os.path.join(tutorial_source_dir, ext)))\n",
    "\n",
    "create_padded_image_dir.save_padded_images(tutorial_image_dir,allfiles,toplot=False,maxwidth=image_width,maxheight=image_height)\n",
    "\n",
    "# TODO: or pick a random set of images to serve as tutorial images\n",
    "N = 6 # number of images to use for tutorials (these will be sampled from to generate subject files below)\n",
    "      # note: make this larger than num_imgs_per_tutorial so not all subject files have the same tutorials\n",
    "    \n",
    "N_sent = 50 # number of sentinels to use for tutorials \n",
    "# note: if equal to num_sentinels_per_tutorial, all subject files will have the same tutorial sentinels\n",
    "\n",
    "generate_tutorials(tutorial_image_dir,rootdir,image_width,image_height,border_padding,N,sentinel_type,sentinel_imdir,N_sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the previous cells have generated the requisite image, codechart, sentinel, and tutorial files, the following code will generate `num_subject_files` individual subject files by sampling from the appropriate image directories and creating an experimental sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3 subject files in bucket 0\n",
      "Subject file ../assets/task_data/subject_files/bucket0/subject_file_0.json DONE\n",
      "Subject file ../assets/task_data/subject_files/bucket0/subject_file_1.json DONE\n",
      "Subject file ../assets/task_data/subject_files/bucket0/subject_file_2.json DONE\n"
     ]
    }
   ],
   "source": [
    "start_subjects_at = 0     # where to start creating subject files at (if had created other subject files previously)\n",
    "#if os.path.exists(os.path.join(rootdir,'subject_files/bucket0')):\n",
    "#    subjfiles = glob.glob(os.path.join(rootdir,'subject_files/bucket0/*.json'))\n",
    "#    start_subjects_at = len(subjfiles)\n",
    "\n",
    "real_codecharts = glob.glob(os.path.join(real_CC_dir,'*.jpg'))\n",
    "sentinel_codecharts = glob.glob(os.path.join(sentinel_CC_dir,'*.jpg'))\n",
    "\n",
    "with open(os.path.join(real_CC_dir,'CC_codes_full.json')) as f:\n",
    "    real_codes_data = json.load(f) # contains mapping of image path to valid codes\n",
    "\n",
    "## GENERATING SUBJECT FILES \n",
    "subjdir = os.path.join(rootdir,'subject_files')\n",
    "if not os.path.exists(subjdir):\n",
    "    os.makedirs(subjdir)\n",
    "    os.makedirs(os.path.join(rootdir,'full_subject_files'))\n",
    "    \n",
    "with open(os.path.join(rootdir,'tutorial_full.json')) as f:\n",
    "    tutorial_data = json.load(f) \n",
    "    \n",
    "tutorial_real_filenames = [fn for fn in tutorial_data.keys() if tutorial_data[fn]['flag']=='tutorial_real']\n",
    "tutorial_sentinel_filenames = [fn for fn in tutorial_data.keys() if tutorial_data[fn]['flag']=='tutorial_sentinel']\n",
    "    \n",
    "# iterate over all buckets \n",
    "for b in range(len(which_buckets)): \n",
    "    \n",
    "    bucket = 'bucket%d'%(which_buckets[b])\n",
    "    img_bucket_dir = os.path.join(real_image_dir,bucket)\n",
    "    img_files = []\n",
    "    for ext in ('*.jpeg', '*.png', '*.jpg'):\n",
    "        img_files.extend(glob.glob(os.path.join(img_bucket_dir, ext)))\n",
    "            \n",
    "    sentinel_bucket_dir = os.path.join(sentinel_image_dir,bucket)\n",
    "    sentinel_files = glob.glob(os.path.join(sentinel_bucket_dir,'*.jpg'))\n",
    "    \n",
    "    with open(os.path.join(sentinel_bucket_dir,'sentinel_codes_full.json')) as f:\n",
    "        sentinel_codes_data = json.load(f) # contains mapping of image path to valid codes\n",
    "        \n",
    "    subjdir = os.path.join(rootdir,'subject_files',bucket)\n",
    "    if not os.path.exists(subjdir):\n",
    "        os.makedirs(subjdir)\n",
    "        os.makedirs(os.path.join(rootdir,'full_subject_files',bucket))\n",
    "    \n",
    "    print('Generating %d subject files in bucket %d'%(num_subject_files,b))\n",
    "    # for each bucket, generate subject files \n",
    "    for i in range(num_subject_files):\n",
    "        \n",
    "        random.shuffle(img_files)\n",
    "        random.shuffle(sentinel_files)\n",
    "        random.shuffle(real_codecharts)\n",
    "        \n",
    "        # for each subject files, add real images \n",
    "        sf_data = []\n",
    "        full_sf_data = []\n",
    "\n",
    "        # ADDING TUTORIALS\n",
    "        random.shuffle(tutorial_real_filenames)\n",
    "        random.shuffle(tutorial_sentinel_filenames)\n",
    "        \n",
    "        # initialize temporary arrays, because will shuffle real & sentinel tutorial images before adding to\n",
    "        # final subject files\n",
    "        sf_data_temp = []\n",
    "        full_sf_data_temp = []\n",
    "        \n",
    "        for j in range(num_imgs_per_tutorial):\n",
    "            \n",
    "            image_data = {}\n",
    "            fn = tutorial_real_filenames[j]\n",
    "            image_data[\"image\"] = fn\n",
    "            image_data[\"codechart\"] = tutorial_data[fn]['codechart_file'] # stores codechart path \n",
    "            image_data[\"codes\"] = tutorial_data[fn]['valid_codes'] # stores valid codes \n",
    "            image_data[\"flag\"] = 'tutorial_real' # stores flag of whether we have real or sentinel image\n",
    "            full_image_data = image_data.copy() # identical to image_data but includes a key for coordinates\n",
    "            full_image_data[\"coordinates\"] = tutorial_data[fn]['coordinates'] # store (x, y) coordinate of each triplet \n",
    "            \n",
    "            sf_data_temp.append(image_data)\n",
    "            full_sf_data_temp.append(full_image_data)\n",
    "        \n",
    "        if add_sentinels_to_tutorial and num_sentinels_per_tutorial>0:\n",
    "            \n",
    "            for j in range(num_sentinels_per_tutorial):\n",
    "                image_data2 = {}\n",
    "                fn = tutorial_sentinel_filenames[j]\n",
    "                image_data2[\"image\"] = fn\n",
    "                image_data2[\"codechart\"] = tutorial_data[fn]['codechart_file'] # stores codechart path \n",
    "                image_data2[\"correct_code\"] = tutorial_data[fn]['correct_code']\n",
    "                image_data2[\"correct_codes\"] = tutorial_data[fn]['correct_codes']\n",
    "                image_data2[\"codes\"] = tutorial_data[fn]['valid_codes'] # stores valid codes \n",
    "                image_data2[\"flag\"] = 'tutorial_sentinel' # stores flag of whether we have real or sentinel image\n",
    "                full_image_data2 = image_data2.copy() # identical to image_data but includes a key for coordinates\n",
    "                full_image_data2[\"coordinate\"] = tutorial_data[fn]['coordinate'] # stores coordinate for correct code\n",
    "                full_image_data2[\"codes\"] = tutorial_data[fn]['valid_codes'] # stores valid codes \n",
    "                full_image_data2[\"coordinates\"] = tutorial_data[fn]['coordinates'] # store (x, y) coordinate of each triplet \n",
    "                \n",
    "                sf_data_temp.append(image_data2)\n",
    "                full_sf_data_temp.append(full_image_data2)\n",
    "                \n",
    "        # up to here, have sequentially added real images and then sentinel images to tutorial\n",
    "        # now want to shuffle them\n",
    "                \n",
    "        perm = np.random.permutation(len(sf_data_temp))\n",
    "        for j in range(len(perm)): # note need to make sure sf_data and full_sf_data correspond\n",
    "            sf_data.append(sf_data_temp[perm[j]])\n",
    "            full_sf_data.append(full_sf_data_temp[perm[j]])\n",
    "        \n",
    "        # ADDING REAL IMAGES \n",
    "        for j in range(num_images_per_sf): \n",
    "            image_data = {}\n",
    "            image_data[\"image\"] = img_files[j] # stores image path \n",
    "\n",
    "            # select a code chart\n",
    "            pathname = real_codecharts[j] # since shuffled, will pick up first set of random codecharts\n",
    "            \n",
    "            image_data[\"codechart\"] = pathname # stores codechart path \n",
    "            image_data[\"codes\"] = real_codes_data[pathname]['valid_codes'] # stores valid codes \n",
    "            image_data[\"flag\"] = 'real' # stores flag of whether we have real or sentinel image\n",
    "            \n",
    "            full_image_data = image_data.copy() # identical to image_data but includes a key for coordinates\n",
    "            full_image_data[\"coordinates\"] = real_codes_data[pathname]['coordinates'] # store locations - (x, y) coordinate of each triplet \n",
    "\n",
    "            sf_data.append(image_data)\n",
    "            full_sf_data.append(full_image_data)\n",
    "\n",
    "        ## ADDING SENTINEL IMAGES \n",
    "        sentinel_spacing = int(num_images_per_sf/float(num_sentinels_per_sf))\n",
    "        insertat = num_imgs_per_tutorial+num_sentinels_per_tutorial + 1; # don't insert before all the tutorial images are done\n",
    "        for j in range(num_sentinels_per_sf):\n",
    "            sentinel_image_data = {}\n",
    "            sentinel_pathname = sentinel_files[j]\n",
    "            sentinel_image_data[\"image\"] = sentinel_pathname # stores image path \n",
    "            sentinel_image_data[\"codechart\"] = sentinel_codes_data[sentinel_pathname]['codechart_file']\n",
    "            sentinel_image_data[\"correct_code\"] = sentinel_codes_data[sentinel_pathname]['correct_code']\n",
    "            sentinel_image_data[\"correct_codes\"] = sentinel_codes_data[sentinel_pathname]['correct_codes']\n",
    "            sentinel_image_data[\"codes\"] = sentinel_codes_data[sentinel_pathname][\"valid_codes\"]\n",
    "            sentinel_image_data[\"flag\"] = 'sentinel' # stores flag of whether we have real or sentinel image\n",
    "            \n",
    "            # for analysis, save other attributes too\n",
    "            full_sentinel_image_data = sentinel_image_data.copy() # identical to sentinel_image_data but includes coordinate key \n",
    "            full_sentinel_image_data[\"coordinate\"] = sentinel_codes_data[sentinel_pathname][\"coordinate\"] # stores the coordinate of the correct code \n",
    "            full_sentinel_image_data[\"codes\"] = sentinel_codes_data[sentinel_pathname][\"valid_codes\"] # stores other valid codes\n",
    "            full_sentinel_image_data[\"coordinates\"] = sentinel_codes_data[sentinel_pathname][\"coordinates\"] # stores the coordinate of the valid code \n",
    "            \n",
    "            insertat = insertat + random.choice(range(sentinel_spacing-1,sentinel_spacing+2))\n",
    "            insertat = min(insertat,len(sf_data)-1)\n",
    "\n",
    "            sf_data.insert(insertat, sentinel_image_data)\n",
    "            full_sf_data.insert(insertat, full_sentinel_image_data)\n",
    "\n",
    "        # Add an image_id to each subject file entry\n",
    "        image_id = 0 # represents the index of the image in the subject file \n",
    "        for d in range(len(sf_data)): \n",
    "            sf_data[d]['index'] = image_id\n",
    "            full_sf_data[d]['index'] = image_id\n",
    "            image_id+=1\n",
    "\n",
    "        subj_num = start_subjects_at+i\n",
    "        with open(os.path.join(rootdir,'subject_files',bucket,'subject_file_%d.json'%(subj_num)), 'w') as outfile: \n",
    "            print('Subject file %s DONE'%(outfile.name))\n",
    "            json.dump(sf_data, outfile)\n",
    "        with open(os.path.join(rootdir,'full_subject_files',bucket,'subject_file_%d.json'%(subj_num)), 'w') as outfile: \n",
    "            json.dump(full_sf_data, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
